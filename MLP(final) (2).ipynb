{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gold', 'platinum', 'palladium', 'tungsten', 'silver', 'y'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold</th>\n",
       "      <th>platinum</th>\n",
       "      <th>palladium</th>\n",
       "      <th>tungsten</th>\n",
       "      <th>silver</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43520</td>\n",
       "      <td>22464</td>\n",
       "      <td>21397</td>\n",
       "      <td>20007</td>\n",
       "      <td>24313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43534</td>\n",
       "      <td>22031</td>\n",
       "      <td>21513</td>\n",
       "      <td>19904</td>\n",
       "      <td>24052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43534</td>\n",
       "      <td>22554</td>\n",
       "      <td>21243</td>\n",
       "      <td>20086</td>\n",
       "      <td>24285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43588</td>\n",
       "      <td>22347</td>\n",
       "      <td>21179</td>\n",
       "      <td>20285</td>\n",
       "      <td>24279</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>22583</td>\n",
       "      <td>21516</td>\n",
       "      <td>19634</td>\n",
       "      <td>24203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23144</td>\n",
       "      <td>21949</td>\n",
       "      <td>20841</td>\n",
       "      <td>20448</td>\n",
       "      <td>24490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27635</td>\n",
       "      <td>22280</td>\n",
       "      <td>21984</td>\n",
       "      <td>19521</td>\n",
       "      <td>24288</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26400</td>\n",
       "      <td>22167</td>\n",
       "      <td>20833</td>\n",
       "      <td>20764</td>\n",
       "      <td>24113</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27318</td>\n",
       "      <td>23069</td>\n",
       "      <td>21733</td>\n",
       "      <td>19636</td>\n",
       "      <td>24339</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26469</td>\n",
       "      <td>21838</td>\n",
       "      <td>20803</td>\n",
       "      <td>20391</td>\n",
       "      <td>24088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27592</td>\n",
       "      <td>22882</td>\n",
       "      <td>21836</td>\n",
       "      <td>19825</td>\n",
       "      <td>24786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27320</td>\n",
       "      <td>21556</td>\n",
       "      <td>21462</td>\n",
       "      <td>20356</td>\n",
       "      <td>23681</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27972</td>\n",
       "      <td>23305</td>\n",
       "      <td>21345</td>\n",
       "      <td>20418</td>\n",
       "      <td>24669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27157</td>\n",
       "      <td>21967</td>\n",
       "      <td>21440</td>\n",
       "      <td>19972</td>\n",
       "      <td>23714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>27672</td>\n",
       "      <td>22802</td>\n",
       "      <td>21156</td>\n",
       "      <td>20011</td>\n",
       "      <td>24526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26952</td>\n",
       "      <td>22019</td>\n",
       "      <td>21381</td>\n",
       "      <td>19878</td>\n",
       "      <td>24304</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27800</td>\n",
       "      <td>22400</td>\n",
       "      <td>21357</td>\n",
       "      <td>19830</td>\n",
       "      <td>24297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>27434</td>\n",
       "      <td>22360</td>\n",
       "      <td>21101</td>\n",
       "      <td>20638</td>\n",
       "      <td>24166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>28458</td>\n",
       "      <td>22514</td>\n",
       "      <td>21637</td>\n",
       "      <td>19685</td>\n",
       "      <td>24032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27093</td>\n",
       "      <td>22005</td>\n",
       "      <td>20652</td>\n",
       "      <td>20327</td>\n",
       "      <td>24523</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gold  platinum  palladium  tungsten  silver  y\n",
       "0   43520     22464      21397     20007   24313  1\n",
       "1   43534     22031      21513     19904   24052  1\n",
       "2   43534     22554      21243     20086   24285  1\n",
       "3   43588     22347      21179     20285   24279  1\n",
       "4      75     22583      21516     19634   24203  1\n",
       "5   23144     21949      20841     20448   24490  1\n",
       "6   27635     22280      21984     19521   24288  1\n",
       "7   26400     22167      20833     20764   24113  1\n",
       "8   27318     23069      21733     19636   24339  1\n",
       "9   26469     21838      20803     20391   24088  1\n",
       "10  27592     22882      21836     19825   24786  1\n",
       "11  27320     21556      21462     20356   23681  1\n",
       "12  27972     23305      21345     20418   24669  1\n",
       "13  27157     21967      21440     19972   23714  1\n",
       "14  27672     22802      21156     20011   24526  1\n",
       "15  26952     22019      21381     19878   24304  1\n",
       "16  27800     22400      21357     19830   24297  1\n",
       "17  27434     22360      21101     20638   24166  1\n",
       "18  28458     22514      21637     19685   24032  1\n",
       "19  27093     22005      20652     20327   24523  1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('combined-dataset.csv')\n",
    "print(dataset.columns)\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoded array:\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(140220, 5)\n",
      "(140220, 13)\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "X = dataset.iloc[:,:5].values\n",
    "y = dataset.iloc[:,5:6].values\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y).toarray()\n",
    "print('One hot encoded array:')\n",
    "print(y[:5])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.4,random_state = 42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4382/4382 [==============================] - 11s 2ms/step - loss: 1.6009 - accuracy: 0.4232\n",
      "Epoch 2/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 1.3171 - accuracy: 0.5176\n",
      "Epoch 3/300\n",
      "4382/4382 [==============================] - 12s 3ms/step - loss: 1.2273 - accuracy: 0.5448\n",
      "Epoch 4/300\n",
      "4382/4382 [==============================] - 12s 3ms/step - loss: 1.1762 - accuracy: 0.5609\n",
      "Epoch 5/300\n",
      "4382/4382 [==============================] - 12s 3ms/step - loss: 1.1446 - accuracy: 0.5713\n",
      "Epoch 6/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 1.1241 - accuracy: 0.5793\n",
      "Epoch 7/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 1.1087 - accuracy: 0.5858\n",
      "Epoch 8/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 1.0948 - accuracy: 0.5890\n",
      "Epoch 9/300\n",
      "4382/4382 [==============================] - 12s 3ms/step - loss: 1.0830 - accuracy: 0.5934 0s - loss: 1\n",
      "Epoch 10/300\n",
      "4382/4382 [==============================] - 12s 3ms/step - loss: 1.0731 - accuracy: 0.5989\n",
      "Epoch 11/300\n",
      "4382/4382 [==============================] - 11s 2ms/step - loss: 1.0620 - accuracy: 0.6033\n",
      "Epoch 12/300\n",
      "4382/4382 [==============================] - 11s 3ms/step - loss: 1.0520 - accuracy: 0.6062\n",
      "Epoch 13/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 1.0447 - accuracy: 0.6105\n",
      "Epoch 14/300\n",
      "4382/4382 [==============================] - 11s 3ms/step - loss: 1.0315 - accuracy: 0.6149\n",
      "Epoch 15/300\n",
      "4382/4382 [==============================] - 11s 2ms/step - loss: 1.0231 - accuracy: 0.6182\n",
      "Epoch 16/300\n",
      "4382/4382 [==============================] - 11s 2ms/step - loss: 1.0146 - accuracy: 0.6229\n",
      "Epoch 17/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 1.0074 - accuracy: 0.6246\n",
      "Epoch 18/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9964 - accuracy: 0.6296\n",
      "Epoch 19/300\n",
      "4382/4382 [==============================] - 11s 2ms/step - loss: 0.9877 - accuracy: 0.6334\n",
      "Epoch 20/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9828 - accuracy: 0.6367\n",
      "Epoch 21/300\n",
      "4382/4382 [==============================] - 11s 3ms/step - loss: 0.9724 - accuracy: 0.6402\n",
      "Epoch 22/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9670 - accuracy: 0.6434\n",
      "Epoch 23/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9591 - accuracy: 0.6467\n",
      "Epoch 24/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9517 - accuracy: 0.6486\n",
      "Epoch 25/300\n",
      "4382/4382 [==============================] - 11s 3ms/step - loss: 0.9455 - accuracy: 0.6523\n",
      "Epoch 26/300\n",
      "4382/4382 [==============================] - 11s 2ms/step - loss: 0.9425 - accuracy: 0.6528\n",
      "Epoch 27/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9390 - accuracy: 0.6544\n",
      "Epoch 28/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9326 - accuracy: 0.6574\n",
      "Epoch 29/300\n",
      "4382/4382 [==============================] - 11s 3ms/step - loss: 0.9266 - accuracy: 0.6592\n",
      "Epoch 30/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9202 - accuracy: 0.6621\n",
      "Epoch 31/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9239 - accuracy: 0.6602\n",
      "Epoch 32/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9199 - accuracy: 0.6628\n",
      "Epoch 33/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9144 - accuracy: 0.6648\n",
      "Epoch 34/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.9135 - accuracy: 0.6641\n",
      "Epoch 35/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.9097 - accuracy: 0.6652\n",
      "Epoch 36/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.9098 - accuracy: 0.6655\n",
      "Epoch 37/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.9032 - accuracy: 0.6681\n",
      "Epoch 38/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8990 - accuracy: 0.6694\n",
      "Epoch 39/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8989 - accuracy: 0.6704\n",
      "Epoch 40/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8992 - accuracy: 0.6685\n",
      "Epoch 41/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8961 - accuracy: 0.6705\n",
      "Epoch 42/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8935 - accuracy: 0.6702\n",
      "Epoch 43/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.8906 - accuracy: 0.6731\n",
      "Epoch 44/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8895 - accuracy: 0.6721\n",
      "Epoch 45/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.8866 - accuracy: 0.6728\n",
      "Epoch 46/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.8839 - accuracy: 0.6747\n",
      "Epoch 47/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.8835 - accuracy: 0.6760\n",
      "Epoch 48/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8826 - accuracy: 0.6748\n",
      "Epoch 49/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.8800 - accuracy: 0.6764\n",
      "Epoch 50/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8779 - accuracy: 0.6765\n",
      "Epoch 51/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8767 - accuracy: 0.6783\n",
      "Epoch 52/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8737 - accuracy: 0.6781\n",
      "Epoch 53/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8730 - accuracy: 0.6792\n",
      "Epoch 54/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8698 - accuracy: 0.6810\n",
      "Epoch 55/300\n",
      "4382/4382 [==============================] - 12s 3ms/step - loss: 0.8685 - accuracy: 0.6797\n",
      "Epoch 56/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8662 - accuracy: 0.6812\n",
      "Epoch 57/300\n",
      "4382/4382 [==============================] - 6s 1ms/step - loss: 0.8656 - accuracy: 0.6812\n",
      "Epoch 58/300\n",
      "4382/4382 [==============================] - 6s 1ms/step - loss: 0.8664 - accuracy: 0.6809\n",
      "Epoch 59/300\n",
      "4382/4382 [==============================] - 6s 1ms/step - loss: 0.8646 - accuracy: 0.6819\n",
      "Epoch 60/300\n",
      "4382/4382 [==============================] - 6s 1ms/step - loss: 0.8656 - accuracy: 0.6810\n",
      "Epoch 61/300\n",
      "4382/4382 [==============================] - 6s 1ms/step - loss: 0.8605 - accuracy: 0.6824\n",
      "Epoch 62/300\n",
      "4382/4382 [==============================] - 6s 1ms/step - loss: 0.8598 - accuracy: 0.6834\n",
      "Epoch 63/300\n",
      "4382/4382 [==============================] - 6s 1ms/step - loss: 0.8600 - accuracy: 0.6833\n",
      "Epoch 64/300\n",
      "4382/4382 [==============================] - 6s 1ms/step - loss: 0.8581 - accuracy: 0.6841\n",
      "Epoch 65/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8584 - accuracy: 0.6842\n",
      "Epoch 66/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8571 - accuracy: 0.6846\n",
      "Epoch 67/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8551 - accuracy: 0.6852\n",
      "Epoch 68/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8526 - accuracy: 0.6865\n",
      "Epoch 69/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8542 - accuracy: 0.6858\n",
      "Epoch 70/300\n",
      "4382/4382 [==============================] - 10s 2ms/step - loss: 0.8531 - accuracy: 0.6865\n",
      "Epoch 71/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8514 - accuracy: 0.6871\n",
      "Epoch 72/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8496 - accuracy: 0.6879\n",
      "Epoch 73/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8476 - accuracy: 0.6874\n",
      "Epoch 74/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8499 - accuracy: 0.6868\n",
      "Epoch 75/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8483 - accuracy: 0.6882\n",
      "Epoch 76/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8458 - accuracy: 0.6890\n",
      "Epoch 77/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8454 - accuracy: 0.6886\n",
      "Epoch 78/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8437 - accuracy: 0.6896\n",
      "Epoch 79/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8414 - accuracy: 0.6907\n",
      "Epoch 80/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8400 - accuracy: 0.6902\n",
      "Epoch 81/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8413 - accuracy: 0.6905\n",
      "Epoch 82/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8387 - accuracy: 0.6916\n",
      "Epoch 83/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8400 - accuracy: 0.6905\n",
      "Epoch 84/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8381 - accuracy: 0.6913\n",
      "Epoch 85/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8394 - accuracy: 0.6911\n",
      "Epoch 86/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8366 - accuracy: 0.6917\n",
      "Epoch 87/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8359 - accuracy: 0.6931\n",
      "Epoch 88/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8375 - accuracy: 0.6923\n",
      "Epoch 89/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8350 - accuracy: 0.6935\n",
      "Epoch 90/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8334 - accuracy: 0.6927\n",
      "Epoch 91/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8327 - accuracy: 0.6942\n",
      "Epoch 92/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8342 - accuracy: 0.6936\n",
      "Epoch 93/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8327 - accuracy: 0.6940\n",
      "Epoch 94/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8317 - accuracy: 0.6940\n",
      "Epoch 95/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8304 - accuracy: 0.6950\n",
      "Epoch 96/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8271 - accuracy: 0.6957\n",
      "Epoch 97/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8281 - accuracy: 0.6953\n",
      "Epoch 98/300\n",
      "4382/4382 [==============================] - 7s 2ms/step - loss: 0.8302 - accuracy: 0.6939\n",
      "Epoch 99/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8273 - accuracy: 0.6953\n",
      "Epoch 100/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8264 - accuracy: 0.6970\n",
      "Epoch 101/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8265 - accuracy: 0.6962\n",
      "Epoch 102/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8256 - accuracy: 0.6962\n",
      "Epoch 103/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8249 - accuracy: 0.6969\n",
      "Epoch 104/300\n",
      "4382/4382 [==============================] - 13s 3ms/step - loss: 0.8263 - accuracy: 0.6960\n",
      "Epoch 105/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8231 - accuracy: 0.6971\n",
      "Epoch 106/300\n",
      "4382/4382 [==============================] - 9s 2ms/step - loss: 0.8235 - accuracy: 0.6968\n",
      "Epoch 107/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8228 - accuracy: 0.6971\n",
      "Epoch 108/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8234 - accuracy: 0.6974\n",
      "Epoch 109/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8207 - accuracy: 0.6995\n",
      "Epoch 110/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8193 - accuracy: 0.6992\n",
      "Epoch 111/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8202 - accuracy: 0.6979\n",
      "Epoch 112/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8223 - accuracy: 0.6972\n",
      "Epoch 113/300\n",
      "4382/4382 [==============================] - 8s 2ms/step - loss: 0.8184 - accuracy: 0.7000\n",
      "Epoch 114/300\n",
      "1709/4382 [==========>...................] - ETA: 4s - loss: 0.8180 - accuracy: 0.69"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(5, input_shape=(5,)))\n",
    "#model.add(keras.layers.Dense(120, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(190, activation='relu'))\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=300)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predications = model.predict(X_test)\n",
    "y_pred = list()\n",
    "for i in range(len(predications)):\n",
    "    y_pred.append(np.argmax(predications[i]))\n",
    "\n",
    "ytest = list()\n",
    "for i in range(len(y_test)):\n",
    "    ytest.append(np.argmax(y_test[i]))\n",
    "\n",
    "print('Accuracy is:', accuracy_score(ytest, y_pred)*100)\n",
    "print('confusion_matrix ',confusion_matrix(ytest,y_pred))\n",
    "print(\"Confusion matrix\",classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
